{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Stoix Results\n",
    "This notebook provides the code to perform the statistical tests and plotting recommended by Rliable. This code expects the JSON files outputted by the Stoix JSON Logger. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the necessary modules\n",
    "Import the necessary modules that will be used for processing the RL experiment data and for producing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from marl_eval.plotting_tools.plotting import (\n",
    "    aggregate_scores,\n",
    "    performance_profiles,\n",
    "    plot_single_task,\n",
    "    probability_of_improvement,\n",
    "    sample_efficiency_curves,\n",
    ")\n",
    "\n",
    "from marl_eval.utils.data_processing_utils import (\n",
    "    create_matrices_for_rliable,\n",
    "    data_process_pipeline,\n",
    ")\n",
    "from marl_eval.utils.diagnose_data_errors import (\n",
    "    DiagnoseData\n",
    ")\n",
    "\n",
    "import collections\n",
    "import json\n",
    "from os import walk\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create JSON file utility functions\n",
    "We create simple functions to find all JSON experiment files in a directory and merge them all together into a single dict. If you desire, you can also save the merged final dictionary to a new JSON for later use instead of storing all JSON files separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dict_from_multirun_folder(\n",
    "    multirun_folder: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Load all json files in a folder and merge them into a single dictionary.\"\"\"\n",
    "   \n",
    "    return load_and_merge_json_dicts(\n",
    "        _get_json_files_from_multirun(multirun_folder)\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_json_files_from_multirun(multirun_folder: str) -> List[str]:\n",
    "    \"\"\"Get all json files in a folder and its subfolders.\"\"\"\n",
    "    files = []\n",
    "    for dirpath, _, filenames in walk(multirun_folder):\n",
    "        for file_name in filenames:\n",
    "            if file_name.endswith(\".json\"):\n",
    "                files.append(str(Path(dirpath) / Path(file_name)))\n",
    "    print(f\"Found {len(files)} json files in {multirun_folder}\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def update_dict(d: Dict[str, Any], u: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively update a dictionary with another dictionary.\"\"\"\n",
    "    for k, v in u.items():\n",
    "        if isinstance(v, collections.abc.Mapping):\n",
    "            d[k] = update_dict(d.get(k, {}), v)  # type: ignore\n",
    "        else:\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "\n",
    "def load_and_merge_json_dicts(\n",
    "    json_input_files: List[str], json_output_file: Optional[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"Load and merge multiple json files into a single dictionary.\"\"\"\n",
    "\n",
    "    dicts = []\n",
    "    json_input_files.sort()\n",
    "    for file in json_input_files:\n",
    "        with open(file, \"r\") as f:\n",
    "            dicts.append(json.load(f))\n",
    "    full_dict: Dict[str, Any] = {}\n",
    "    for single_dict in dicts:\n",
    "        update_dict(full_dict, single_dict)\n",
    "\n",
    "    if json_output_file is not None:\n",
    "        with open(json_output_file, \"w+\") as f:\n",
    "            json.dump(full_dict, f, indent=4)\n",
    "\n",
    "    return full_dict\n",
    "\n",
    "\n",
    "def read_json_file(file_path: str) -> Any:\n",
    "    \"\"\"Read a json file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Specify the folder directory of all your JSON experiment results\n",
    "Insert the folder directory of the results as well as which specific environment suite you want to plot. We specify the environment suite (not task) as its possible you are storing all results in the same folder. An example of an environment suite is \"Atari\" where a task is \"Breakout\". Another example is \"Brax\" where a task is \"Ant\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER_DIR = \"../results\" # Path to the folder containing the results - absolute path might be needed\n",
    "ENVIRONMENT_SUITE = \"\" # e.g. \"brax\" or \"gymnax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load and process the JSON files\n",
    "\n",
    "As long as everything is valid (except the metrics dont need to be perfectly valid in the diagnosis if the overlapping metrics contain \"mean_episode_return\") then all plotting should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "raw_dict = get_raw_dict_from_multirun_folder(\n",
    "        multirun_folder=RESULTS_FOLDER_DIR,\n",
    "    )\n",
    "\n",
    "# Choose the list of metrics to normalize\n",
    "METRICS_TO_NORMALIZE = [\"mean_episode_return\"]\n",
    "\n",
    "# Call data_process_pipeline to normalize the chosen metrics and to clean the data\n",
    "print(DiagnoseData(raw_dict).check_data())\n",
    "processed_data = data_process_pipeline(\n",
    "    raw_data=raw_dict, metrics_to_normalize=METRICS_TO_NORMALIZE\n",
    ")\n",
    "\n",
    "# Create a custom legend mapping dictionary for the plotting functions\n",
    "# This dictionary maps the algorithm names in the data to the names that will be displayed in the plots\n",
    "LEGEND_MAP = None # e.g. {\"ff_ppo\" : \"PPO\", \"ff_mpo\" : \"MPO\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create two different dictionaries that will be used to create plots. To do this we will make use of the [`create_matrices_for_rliable`](https://github.com/instadeepai/marl-eval/blob/4856b06467872fd8b6f8a5db5fa7c9d7d55e2431/marl_eval/utils/data_processing_utils.py#L279) function. \n",
    "  \n",
    "  **a)  metric_dictionary_return:**\n",
    "\n",
    "  - The dictionary will have the names of all aggregated metrics as keys. Under each aggregated metric key there will be a 2D array corresponding to the mean of the normalised absolute metric values for each algorithm in a given experiment. \n",
    "\n",
    "  - These 2D arrays will have dimensions given as (number of runs x number of tasks). \n",
    "\n",
    "  **b)  final_metric_tensor_dictionary:**\n",
    "\n",
    "  - This dictionary will, similarly, have keys corresponding to the aggregated experiment metrics. Under each aggregated metric key there will now be a 3D array corresponding to the mean normalised metric at each evaluation step. \n",
    "\n",
    "  - These 3D arrays will have dimensions given as (number of runs x number of tasks x number of logging steps).\n",
    "\n",
    "The `create_matrices_for_rliable` function has 3 arguments which are:\n",
    "\n",
    "  - data_dictionary: The dictionary of processed data\n",
    "  \n",
    "  - environment_name: Name of environment for which the arrays should be computed.\n",
    "\n",
    "  - metrics_to_normalize: List of metric metric names that should be normalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_comparison_matrix, sample_efficiency_matrix = create_matrices_for_rliable(\n",
    "    data_dictionary=processed_data,\n",
    "    environment_name=ENVIRONMENT_SUITE,\n",
    "    metrics_to_normalize=METRICS_TO_NORMALIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Produce the Plots:\n",
    "1. Performance Profiles\n",
    "2. Aggregate Point Scores\n",
    "3. Sample Efficiency\n",
    "4. Probability of Improvement\n",
    "5. Individual Task Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = performance_profiles(\n",
    "    environment_comparison_matrix,\n",
    "    metric_name=\"mean_episode_return\",\n",
    "    metrics_to_normalize=METRICS_TO_NORMALIZE,\n",
    "    legend_map=LEGEND_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _, _ = aggregate_scores(\n",
    "    dictionary=environment_comparison_matrix,\n",
    "    metric_name=\"mean_episode_return\",\n",
    "    metrics_to_normalize=METRICS_TO_NORMALIZE,\n",
    "    save_tabular_as_latex=False,\n",
    "    legend_map=LEGEND_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _, _ = sample_efficiency_curves(\n",
    "    dictionary=sample_efficiency_matrix,\n",
    "    metric_name=\"mean_episode_return\",\n",
    "    metrics_to_normalize=METRICS_TO_NORMALIZE,\n",
    "    legend_map=LEGEND_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHM_TO_COMPARE = \"\" # e.g. \"ff_ppo\"\n",
    "found_algorithm_names = [algo.lower() for algo in environment_comparison_matrix[\"mean_norm_mean_episode_return\"].keys()]\n",
    "found_algorithm_names.remove(ALGORITHM_TO_COMPARE)\n",
    "algorithms_to_compare = [[ALGORITHM_TO_COMPARE] + [algorithm_name] for algorithm_name in found_algorithm_names]\n",
    "\n",
    "fig = probability_of_improvement(\n",
    "    environment_comparison_matrix,\n",
    "    metric_name=\"mean_episode_return\",\n",
    "    metrics_to_normalize=METRICS_TO_NORMALIZE,\n",
    "    algorithms_to_compare=algorithms_to_compare,\n",
    "    legend_map=LEGEND_MAP,\n",
    ")\n",
    "plt.gca().set_xlim([-0.1, 1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in processed_data[ENVIRONMENT_SUITE]:\n",
    "    fig = plot_single_task(\n",
    "        processed_data=processed_data,\n",
    "        environment_name=ENVIRONMENT_SUITE,\n",
    "        task_name=task,\n",
    "        metric_name=\"mean_episode_return\",\n",
    "        metrics_to_normalize=[], # Put in METRICS_TO_NORMALIZE if you want the normalised scores per task,\n",
    "        legend_map=LEGEND_MAP,\n",
    "    )\n",
    "    plt.title(f\"{task.capitalize()}\", fontsize=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
