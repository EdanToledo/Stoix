# --- Defaults FF-PPO ---

system_name: ff_ppo # Name of the system.

# --- RL hyperparameters ---
actor_lr: 3e-4 # Learning rate for actor network
critic_lr: 3e-4 # Learning rate for critic network
rollout_length: 37 # Number of environment steps per vectorised environment.
epochs: 4 # Number of ppo epochs per training data batch.
num_minibatches: 16 # Number of minibatches per ppo epoch.
gamma: 1.0 # Discounting factor.
gae_lambda: 1.0 # Lambda value for GAE computation.
clip_eps: 0.2 # Clipping value for PPO updates and value function.
ent_coef: 0.001 # Entropy regularisation term for loss function.
vf_coef: 1.0 # Critic weight in
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
standardize_advantages: True # Whether to standardize the advantages.
kl_penalty_coef: 3.0 # KL penalty coefficient for PPO updates if using PPO Penalty.
redistribute_reward: False # Whether to redistribute episodic rewards across all transitions
redistribute_reward_implicit: False # Whether to implicitly redistribute episodic rewards across all transitions (scales advantage by (T-t)/T)
disable_autoreset: True # Whether to disable Stoix-level autoresetting of environments (on env termination during rollout)
use_mc_returns_ae: True # Whether to use Monte-Carlo returns for advantage estimation
