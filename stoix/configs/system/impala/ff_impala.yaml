# --- Defaults FF-IMPALA ---

system_name: ff_impala # Name of the system.

# --- RL hyperparameters ---
actor_lr: 3e-4 # Learning rate for actor network
critic_lr: 3e-4 # Learning rate for critic network
rollout_length: 20 # Number of environment steps per rollout
gamma: 0.997 # Discounting factor
vtrace_lambda: 0.95 # Lambda parameter for V-trace (paper uses 1.0)
clip_rho_threshold: 1.0 # Importance weight clipping threshold for value correction (ρ̄)
clip_pg_rho_threshold: 1.0 # Importance weight clipping threshold for policy gradient (c̄)
ent_coef: 0.001 # Entropy regularisation term for loss function
vf_coef: 1.0 # Value function coefficient in loss
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update
normalize_rewards: False # Whether to normalize rewards (as done in paper)
reward_scale: 1.0 # Scale factor for reward normalization
reward_eps: 1e-8 # Epsilon for reward normalization numerical stability
