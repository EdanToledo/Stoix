# --- Defaults FF-MPO Continuous ---

system_name: ff_mpo # Name of the system.

# --- RL hyperparameters ---
rollout_length: 8 # Number of environment steps per vectorised environment.
epochs: 72 # Number of sgd steps per rollout.
warmup_steps: 16  # Number of steps to collect before training.
total_buffer_size: 200_000 # Total effective size of the replay buffer across all devices and vectorised update steps. This means each device has a buffer of size buffer_size//num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
total_batch_size: 256 # Total effective number of samples to train on. This means each device has a batch size of batch_size/num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
sample_sequence_length: 16 # Number of steps to consider for each element of the batch.
period : 1 # Period of the sampled sequences.
actor_lr: 1e-4  # the learning rate of the policy network optimizer
q_lr: 1e-4  # the learning rate of the Q network network optimizer
dual_lr: 1e-3  # the learning rate of the alpha optimizer
tau: 0.005  # smoothing coefficient for target networks
gamma: 0.99  # discount factor
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
max_abs_reward : 20_000  # maximum absolute reward value
num_samples: 128 # Number of MPO action samples for the policy update.
epsilon: 0.05 # KL constraint on the non-parametric auxiliary policy, the one associated with the dual variable called temperature.
epsilon_mean : 0.05 # KL constraint on the mean of the Gaussian policy, the one associated with the dual variable called alpha_mean.
epsilon_stddev: 0.0005 # KL constraint on the stddev of the Gaussian policy, the one associated with the dual variable called alpha_mean.
init_log_temperature: 10. # initial value for the temperature in log-space, note a softplus (rather than an exp) will be used to transform this.
init_log_alpha_mean: 10. # initial value for the alpha_mean in log-space, note a softplus (rather than an exp) will be used to transform this.
init_log_alpha_stddev: 500. # initial value for the alpha_stddev in log-space, note a softplus (rather than an exp) will be used to transform this.
per_dim_constraining: True # whether to enforce the KL constraint on each dimension independently; this is the default. Otherwise the overall KL is constrained, which allows some dimensions to change more at the expense of others staying put.
stochastic_policy_eval: True # whether to use a stochastic policy for Q function target evaluation.
use_online_policy_to_bootstrap: False # whether to use the online policy to bootstrap the Q function targets.
use_retrace : False # whether to use the retrace algorithm for off-policy correction.
retrace_lambda : 0.95 # the retrace lambda parameter.
use_n_step_bootstrap : False # whether to use n-step bootstrapping for the Q function targets.
n_step_for_sequence_bootstrap : 5 # the number of steps to use for the sequence bootstrap. This is only used if use_retrace is False and use_n_step_bootstrap is True.
gae_lambda : 0.95 # the GAE lambda parameter. This is only used if use_retrace is False and use_n_step_bootstrap is False.
