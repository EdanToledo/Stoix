# --- Defaults Rec-R2D2 ---

system_name: r2d2 # Name of the system.

# --- RL hyperparameters ---
rollout_length: 40 # Number of environment steps per vectorised environment.
epochs: 32 # Number of sgd steps per rollout.
warmup_steps: 128 # Number of steps to collect before training.
total_buffer_size: 100_000 # Total effective size of the replay buffer across all devices and vectorised update steps. This means each device has a buffer of size buffer_size//num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
total_batch_size: 32 # Total effective number of samples to train on. This means each device has a batch size of batch_size/num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
burn_in_length: 40 # Number of steps to burn in before training.
sample_sequence_length: 80 # Length of the sequence to sample from the buffer.
period: 40 # Amount of trajectory overlap
priority_exponent: 0.9 # exponent for the prioritised experience replay
importance_sampling_exponent: 0.6 # exponent for the importance sampling weights
priority_eta: 0.9  # Balance between max and mean priorities
n_step: 5 # how many steps in the transition to use for the n-step return
q_lr: 1e-4 # the learning rate of the Q network network optimizer
tau: 0.005 # smoothing coefficient for target networks
gamma: 0.99 # discount factor
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
training_epsilon: 0.1 # epsilon for the epsilon-greedy policy during training
evaluation_epsilon: 0.0 # epsilon for the epsilon-greedy policy during evaluation
max_abs_reward: 1000.0 # maximum absolute reward value
