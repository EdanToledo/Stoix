# --- Defaults FF-SAC ---

system_name: ff_sac # Name of the system.

# --- RL hyperparameters ---
update_batch_size: 1 # Number of vectorised gradient updates per device.
rollout_length: 1 # Number of environment steps per vectorised environment.
epochs: 1 # Number of sgd steps per rollout.
warmup_steps: 64  # Number of steps to collect before training.
total_buffer_size: 100_000 # Total effective size of the replay buffer across all devices. This means each device has a buffer of size buffer_size/num_devices. This must be divisible by num_devices.
total_batch_size: 256 # Total effective number of samples to train on. This means each device has a batch size of batch_size/num_devices. This must be divisible by num_devices.
actor_lr: 3e-4  # the learning rate of the policy network optimizer
q_lr: 3e-4  # the learning rate of the Q network network optimizer
alpha_lr: 3e-4  # the learning rate of the alpha optimizer
tau: 0.005  # smoothing coefficient for target networks
gamma: 0.99  # discount factor
autotune: True  # whether to autotune alpha
target_entropy_scale: 1.0  # scale factor for target entropy (when auto-tuning)
init_alpha: 0.1  # initial entropy value when not using autotune
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
