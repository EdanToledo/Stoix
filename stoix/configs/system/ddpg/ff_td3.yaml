# --- Defaults FF-Td3 ---

system_name: ff_td3 # Name of the system.

# --- RL hyperparameters ---
rollout_length: 1 # Number of environment steps per vectorised environment.
epochs: 1 # Number of sgd steps per rollout.
warmup_steps: 1000  # Number of steps to collect before training.
total_buffer_size: 1_000_000 # Total effective size of the replay buffer across all devices and vectorised update steps. This means each device has a buffer of size buffer_size//num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
total_batch_size: 256 # Total effective number of samples to train on. This means each device has a batch size of batch_size/num_devices which is further divided by the update_batch_size. This value must be divisible by num_devices*update_batch_size.
actor_lr: 1e-4  # the learning rate of the policy network optimizer
q_lr: 1e-4  # the learning rate of the Q network network optimizer
tau: 0.005  # smoothing coefficient for target networks
gamma: 0.99  # discount factor
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: True # Whether learning rates should be linearly decayed during training.
max_abs_reward : 20_000  # maximum absolute reward value
exploration_noise : 0.15  # standard deviation of the exploration noise
policy_noise: 0.2 # standard deviation of the policy noise
policy_frequency: 2 # frequency of the policy update in the TD3 algorithm (delayed policy update)
noise_clip: 0.5 # noise clip parameter of the Target Policy Smoothing Regularization
