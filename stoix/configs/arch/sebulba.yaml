# --- Sebulba config ---
architecture_name : sebulba
# --- Training ---
seed: 42  # RNG seed.
total_num_envs: 120  # Total Number of vectorised environments across all actors. Needs to be divisible by the number of actor devices and actors per device.
total_timesteps: 1e7 # Set the total environment steps.
# If unspecified, it's derived from num_updates; otherwise, num_updates adjusts based on this value.
num_updates: ~ # Number of updates

# Define the number of actors per device and which devices to use.
actor:
  device_ids: [0] # Define which devices to use for the actors.
  actor_per_device: 2 # number of different threads per actor device.
  log_frequency: 10 # How often to log actor statistics.

# Define which devices to use for the learner.
learner:
  device_ids: [1,2,3,4,5,6] # Define which devices to use for the learner.
  log_frequency: 10 # How often to log learner statistics.

# Whether the actors wait for the learner to finish their update before starting the next rollout.
synchronous: False

# Define which device to use for the evaluator.
evaluator_device_id: 7

# --- Evaluation ---
evaluation_greedy: False # Evaluate the policy greedily. If True the policy will select
  # an action which corresponds to the greatest logit. If false, the policy will sample
  # from the logits.
num_eval_episodes: 60 # Number of episodes to evaluate per evaluation.
num_evaluation: 10 # Number of evenly spaced evaluations to perform during training.
absolute_metric: False # Whether the absolute metric should be computed. For more details
  # on the absolute metric please see: https://arxiv.org/abs/2209.10485
