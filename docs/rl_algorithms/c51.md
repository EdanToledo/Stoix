# Categorical DQN (C51)

| :material-file-document: Paper      |:material-github: code |
| ----------- | ----------- |
|*[Bellemare et al. (2017)](https://arxiv.org/abs/1707.06887)*| [`categorical_dqn.py`](https://github.com/EdanToledo/Stoix/blob/main/stoix/systems/q_learning/ff_c51.py) |

## Key features

* **Distributional RL**: Categorical DQN (C51) is a distributional reinforcement learning algorithm that models the distribution of the return \( Z(s, a) \) instead of just its expectation \( Q(s, a) \). This allows the agent to capture the inherent uncertainty and variability in the rewards.

* **Discrete Support**: C51 represents the return distribution using a fixed set of atoms (discrete support) \( z_i \) within a range \([v_{\text{min}}, v_{\text{max}}]\). The atoms are uniformly spaced, and the probabilities associated with these atoms form the distribution.

* **Projection onto Support**: To compute the target distribution, C51 projects the Bellman update onto the fixed support of atoms. The target distribution is given by:

$$
\hat{T}Z(s, a) = R(s, a) + \gamma Z(s', a')
$$

where \( R(s, a) \) is the reward, \( \gamma \) is the discount factor, and \( Z(s', a') \) is the distribution of the next state-action pair.

* **Categorical Loss**: The loss function used in C51 is the Kullback-Leibler (KL) divergence between the projected target distribution and the current estimated distribution. This encourages the learned distribution to match the target distribution as closely as possible.

## Algorithm

1. **Initialize**:
    - Initialize the Q-network with random weights \( \theta \).
    - Initialize the target Q-network with weights \( \theta^- = \theta \).
    - Initialize the replay buffer \( D \).
    - Define the atom support \( z_i \) within the range \([v_{\text{min}}, v_{\text{max}}]\).

2. **Interaction with Environment**:
    - For each episode:
        - For each time step:
            - With probability \( \epsilon \), select a random action \( a \).
            - Otherwise, select the action \( a = \arg\max \mathbb{E}[Z(s, a; \theta)] \).
            - Execute action \( a \) and observe reward \( r \) and next state \( s' \).
            - Store transition \( (s, a, r, s') \) in the replay buffer \( D \).

3. **Training**:
    - Sample a mini-batch of transitions \( (s, a, r, s') \) from the replay buffer \( D \).
    - Compute the target distribution by projecting the Bellman update onto the fixed support:

$$
\hat{T}Z(s, a) = R(s, a) + \gamma Z(s', a')
$$

- Minimize the KL divergence between the projected target distribution and the estimated distribution using gradient descent:

$$
L(\theta) = \mathbb{E}_{(s, a, r, s') \sim D} \left[ \text{KL}(\hat{T}Z(s, a) \parallel Z(s, a; \theta)) \right]
$$

4. **Update Target Network**:
    - Every \( C \) steps, update the target network: \( \theta^- = \theta \).
    - Alternatively, use Polyak averaging to update the target network incrementally: \( \theta^- \leftarrow \tau \theta + (1 - \tau) \theta^- \), where \( \tau \in [0, 1] \) is the Polyak averaging factor.

## Advantages
1. **Better Uncertainty Modeling**: By modeling the distribution of returns, C51 provides a more nuanced understanding of uncertainty and variability in the environment, leading to better decision-making.

2. **Stability and Robustness**: Distributional approaches like C51 tend to be more stable and robust compared to traditional Q-learning methods, as they capture more information about the environment.

3. **Improved Performance**: C51 has demonstrated superior performance on various benchmarks, particularly in complex environments with high variability in rewards.
